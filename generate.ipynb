{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe194ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imports...\n",
      "\n",
      "Configured optimizer with weight decay\n",
      "- num decayed param tensors: 50, totaling 124,354,560 parameters\n",
      "- num non-decayed parameter tensors: 98, totaling 121,344 parameters\n",
      "- fused AdamW: False\n",
      "\n",
      "Loaded model from train step 19073 with val loss 3.09375\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading imports...\\n\")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tiktoken\n",
    "import random\n",
    "\n",
    "from train_gpt2 import GPT, GPTConfig, load_model_from_save\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304)).to(device)\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "_, _ = load_model_from_save(model, \"log/model_19072.pt\", optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d80f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, device, num_return_sequences=1, max_new_tokens=50, temp=0.8, top_k = 50):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(random.randint(0, 2**32 - 1))\n",
    "    # sample_rng.manual_seed(42) # get different results for each process w/o affecting global rng \n",
    "\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = enc.encode(prompt)\n",
    "\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long) # (sequence_length, )\n",
    "    tokens = tokens.unsqueeze(dim=0).repeat(num_return_sequences, 1) # (num_return_sequences, sequence_length)\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    for t in range(max_new_tokens):\n",
    "        with torch.inference_mode():\n",
    "            logits, _ = model(x) # (B, T, vocab_size)\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "            if temp == 0:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            else:\n",
    "                logits = logits / temp\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1) # (B, 50)\n",
    "                # select a token\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "                next_token = torch.gather(topk_indices, -1, ix) # get the element at the index for each batch\n",
    "                x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "        if num_return_sequences == 1:\n",
    "            token_id = next_token[0, 0].item()\n",
    "            decoded_token = enc.decode([token_id])\n",
    "\n",
    "            if t == 0:\n",
    "                print(f\"{prompt}{decoded_token}\", end=\"\")\n",
    "            else:\n",
    "                print(decoded_token, end=\"\")\n",
    "\n",
    "    if num_return_sequences > 1:\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = x[i].tolist()\n",
    "            decoded = enc.decode(tokens)\n",
    "            print(f\"\\nSequence #{i+1}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e17d0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text...\n",
      "I think that quantum mechanics is the foundation of a bunch of interesting things. We are looking at quantum computing and in physics quantum dynamics is something that you can just say, \"well we're building it, but we're really exploring things in the opposite direction.\" So it may be some form of a very small but amazing world that there are things happening in the unknown, perhaps even some strange world. And it all comes down to what we're hoping to figure out here, maybe maybe maybe someday we can be able to help build"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating text...\")\n",
    "\n",
    "# question, answer based prompts\n",
    "prompt1 = \"Question: Who wrote the play 'Romeo and Juliet'?\\n\"\\\n",
    "          \"Answer: William Shakespeare\\n\"\\\n",
    "          \"Question: What is the capital of Canada?\\n\"\\\n",
    "          \"Answer:\"\n",
    "\n",
    "# generate_text(model, prompt1, device=device, temp=0.2, top_k=40, max_new_tokens=8)\n",
    "\n",
    "prompt2 = \"Question: What is the largest planet in our solar system?\\n\"\\\n",
    "          \"Answer:\"\n",
    "# generate_text(model, prompt2, device=device, temp=0.1, top_k=40, max_new_tokens=10)\n",
    "\n",
    "\n",
    "\n",
    "# summarization prompt\n",
    "\n",
    "# not exactly summarizes and more using information memorized from fineweb data\n",
    "prompt4 = \"\"\"Article:  \n",
    "The Amazon rainforest, often referred to as the \"lungs of the Earth,\" produces around 20% of the world's oxygen. It is home to millions of species of plants and animals, many of which are found nowhere else. In recent decades, deforestation caused by logging, agriculture, and mining has threatened the biodiversity and climate stability provided by this vast ecosystem. Efforts by governments, NGOs, and indigenous groups continue to focus on conservation and sustainable development.\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "# generate_text(model, prompt4, device=device, temp=0.3, top_k=40, max_new_tokens=50)\n",
    "\n",
    "\n",
    "\n",
    "# passage + Q & A\n",
    "# works sometimes\n",
    "# other times it completely doesn't answer the question or gives an answer not based on the passage\n",
    "\n",
    "prompt5 = \"\"\"\n",
    "Article: Mount Everest is the highest mountain in the world, located between Nepal and China. The first confirmed ascent was made by Sir Edmund Hillary and Tenzing Norgay in 1953.\n",
    "Q1: Where is Mount Everest located?\n",
    "A1: Between Nepal and China\n",
    "\n",
    "Q2: Who were the first climbers to reach the summit?\n",
    "A2:\"\"\"\n",
    "\n",
    "# Gives incorrect answer if no passage provided\n",
    "# prompt5 = \"\"\"\n",
    "# Question: Who were the first climbers to reach the summit of Mount Everest?\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# generate_text(model, prompt5, device=device, temp=0.2, top_k=40, max_new_tokens=20)\n",
    "\n",
    "\n",
    "\n",
    "# creative text completion (my fav)\n",
    "prompt6 = \"I think that quantum mechanics is\"\n",
    "generate_text(model, prompt6, device=device, temp=1.2, max_new_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e01dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
