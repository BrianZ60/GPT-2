{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe194ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imports...\n",
      "\n",
      "Configured optimizer with weight decay\n",
      "- num decayed param tensors: 50, totaling 124,354,560 parameters\n",
      "- num non-decayed parameter tensors: 98, totaling 121,344 parameters\n",
      "- fused AdamW: False\n",
      "\n",
      "Loaded model from train step 19073 with val loss 3.09375\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading imports...\\n\")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tiktoken\n",
    "import random\n",
    "\n",
    "from train_gpt2 import GPT, GPTConfig, load_model_from_save\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304)).to(device)\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "_, _ = load_model_from_save(model, \"log/model_19072.pt\", optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d80f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "        model,\n",
    "        prompt,\n",
    "        device,\n",
    "        num_return_sequences=1,\n",
    "        max_new_tokens=50,\n",
    "        temp=0.8,\n",
    "        top_k = 50\n",
    "    ):\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        sample_rng = torch.Generator(device=device)\n",
    "        sample_rng.manual_seed(random.randint(0, 2**32 - 1))\n",
    "        # sample_rng.manual_seed(42) # get different results for each process w/o affecting global rng \n",
    "\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        tokens = enc.encode(prompt)\n",
    "\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long) # (sequence_length, )\n",
    "        tokens = tokens.unsqueeze(dim=0).repeat(num_return_sequences, 1) # (num_return_sequences, sequence_length)\n",
    "        x = tokens.to(device)\n",
    "\n",
    "        for t in range(max_new_tokens):\n",
    "            with torch.inference_mode():\n",
    "                logits, _ = model(x) # (B, T, vocab_size)\n",
    "                logits = logits[:, -1, :] # (B, vocab_size)\n",
    "                logits = logits / temp\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "                topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1) # (B, 50)\n",
    "\n",
    "                # select a token\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "                xcol = torch.gather(topk_indices, -1, ix) # get the element at the index for each batch\n",
    "                # xcol = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "                x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "            if num_return_sequences == 1:\n",
    "                token_id = xcol[0, 0].item()\n",
    "                decoded_token = enc.decode([token_id])\n",
    "\n",
    "                if t == 0:\n",
    "                    print(f\"{prompt}{decoded_token}\", end=\"\")\n",
    "                else:\n",
    "                    print(decoded_token, end=\"\")\n",
    "\n",
    "        if num_return_sequences > 1:\n",
    "            for i in range(num_return_sequences):\n",
    "                tokens = x[i].tolist()\n",
    "                decoded = enc.decode(tokens)\n",
    "                print(f\"\\nSequence #{i+1}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17d0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text...\n",
      "I think that Artifical Intelligence (AI) is the future (if I can make that argument) because of its use in a business context and its use in everyday life.\n",
      "AI is already used for other purposes, such as helping businesses solve complex business problems.\n",
      "But how can it assist in a business context?\n",
      "AI can assist in a business context by facilitating:\n",
      "- Decision making by making decisions by comparing and contrasting information\n",
      "- Analyzing information with data that is meaningful for you, or better yet, which is still only useful for"
     ]
    }
   ],
   "source": [
    "prompt = \"I think that Artifical Intelligence (AI) is\"\n",
    "\n",
    "print(\"\\nGenerating text...\")\n",
    "# model.generate_text(prompt, device=device, num_return_sequences=3, max_new_tokens=30)\n",
    "generate_text(model, prompt, device=device, max_new_tokens=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
