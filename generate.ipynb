{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe194ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imports...\n",
      "\n",
      "Configured optimizer with weight decay\n",
      "- num decayed param tensors: 50, totaling 124,354,560 parameters\n",
      "- num non-decayed parameter tensors: 98, totaling 121,344 parameters\n",
      "- fused AdamW: False\n",
      "\n",
      "Loaded model from train step 19073 with val loss 3.09375\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading imports...\\n\")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tiktoken\n",
    "import random\n",
    "\n",
    "from train_gpt2 import GPT, GPTConfig, load_model_from_save\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304)).to(device)\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "_, _ = load_model_from_save(model, \"log/model_19072.pt\", optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d80f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, device, num_return_sequences=1, max_new_tokens=50, temp=0.8, top_k = 50):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(random.randint(0, 2**32 - 1))\n",
    "    # sample_rng.manual_seed(42) # get different results for each process w/o affecting global rng \n",
    "\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = enc.encode(prompt)\n",
    "\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long) # (sequence_length, )\n",
    "    tokens = tokens.unsqueeze(dim=0).repeat(num_return_sequences, 1) # (num_return_sequences, sequence_length)\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    for t in range(max_new_tokens):\n",
    "        with torch.inference_mode():\n",
    "            logits, _ = model(x) # (B, T, vocab_size)\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "            if temp == 0:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            else:\n",
    "                logits = logits / temp\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1) # (B, 50)\n",
    "                # select a token\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "                next_token = torch.gather(topk_indices, -1, ix) # get the element at the index for each batch\n",
    "                x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "        if num_return_sequences == 1:\n",
    "            token_id = next_token[0, 0].item()\n",
    "            decoded_token = enc.decode([token_id])\n",
    "\n",
    "            if t == 0:\n",
    "                print(f\"{prompt}{decoded_token}\", end=\"\")\n",
    "            else:\n",
    "                print(decoded_token, end=\"\")\n",
    "\n",
    "    if num_return_sequences > 1:\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = x[i].tolist()\n",
    "            decoded = enc.decode(tokens)\n",
    "            print(f\"\\nSequence #{i+1}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e17d0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text...\n",
      "I think that artifical intelligence (AI) is a very effective and useful idea that��s not currently happening in many areas and isn��t going to be able to change the way we approach work in future. An AI that could replace or improve an old machine tool might not be able to. However, artificial intelligence will not stop and change the way technology should be used because our society will continue to use it. As someone who had once described this machine thinking thinking concept as ��innovation��, it just took a little"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating text...\")\n",
    "\n",
    "# question, answer based prompts\n",
    "prompt1 = \"Question: Who wrote the play 'Romeo and Juliet'?\\n\"\\\n",
    "          \"Answer: William Shakespeare\\n\"\\\n",
    "          \"Question: What is the capital of Canada?\\n\"\\\n",
    "          \"Answer:\"\n",
    "\n",
    "# generate_text(model, prompt1, device=device, temp=0.2, top_k=40, max_new_tokens=8)\n",
    "\n",
    "prompt2 = \"Question: What is the largest planet in our solar system?\\n\"\\\n",
    "          \"Answer:\"\n",
    "# generate_text(model, prompt2, device=device, temp=0.1, top_k=40, max_new_tokens=10)\n",
    "\n",
    "\n",
    "\n",
    "# summarization prompt\n",
    "\n",
    "# not exactly summarizes and more using information memorized from fineweb data\n",
    "prompt3 = \"\"\"Article:  \n",
    "The Amazon rainforest, often referred to as the \"lungs of the Earth,\" produces around 20% of the world's oxygen. It is home to millions of species of plants and animals, many of which are found nowhere else. In recent decades, deforestation caused by logging, agriculture, and mining has threatened the biodiversity and climate stability provided by this vast ecosystem. Efforts by governments, NGOs, and indigenous groups continue to focus on conservation and sustainable development.\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "# generate_text(model, prompt3, device=device, temp=0.3, top_k=40, max_new_tokens=50)\n",
    "\n",
    "\n",
    "\n",
    "# passage + Q & A\n",
    "# works sometimes\n",
    "# other times it completely doesn't answer the question or gives an answer not based on the passage\n",
    "\n",
    "prompt4 = \"\"\"\n",
    "Article: Mount Everest is the highest mountain in the world, located between Nepal and China. The first confirmed ascent was made by Sir Edmund Hillary and Tenzing Norgay in 1953.\n",
    "Q1: Where is Mount Everest located?\n",
    "A1: Between Nepal and China\n",
    "\n",
    "Q2: Who were the first climbers to reach the summit?\n",
    "A2:\"\"\" \n",
    "\n",
    "# Gives incorrect answer if no passage provided\n",
    "# prompt5 = \"\"\"\n",
    "# Question: Who were the first climbers to reach the summit of Mount Everest?\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# generate_text(model, prompt4, device=device, temp=0.2, top_k=40, max_new_tokens=20)\n",
    "\n",
    "\n",
    "\n",
    "# creative text completion (my fav)\n",
    "prompt5 = \"I think that artifical intelligence (AI) is\"\n",
    "generate_text(model, prompt5, device=device, temp=1.2, max_new_tokens=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
